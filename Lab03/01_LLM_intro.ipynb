{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdf12054",
   "metadata": {},
   "source": [
    "# üåü LLM\n",
    "## What is an LLM?\n",
    "\n",
    "**LLM** stands for **Large Language Model**.\n",
    "It is a type of artificial intelligence trained on huge amounts of text so it can understand and generate human-like language.\n",
    "\n",
    "‚úîÔ∏è Think of an LLM as:\n",
    "\n",
    "A super-advanced text-prediction machine.\n",
    "Similar to how your phone predicts the next word when you type, but thousands of times more powerful.\n",
    "\n",
    "‚úîÔ∏è What can an LLM do?\n",
    "\n",
    "- Answer questions\n",
    "- Summarize text\n",
    "- Translate languages\n",
    "- Write code\n",
    "- Have conversations\n",
    "- Help with homework or explanations\n",
    "\n",
    "‚úîÔ∏è Why ‚Äúlarge‚Äù?\n",
    "\n",
    "Because it uses billions of parameters (mathematical patterns learned from text). The more parameters, usually the more capable the model is."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17ff1fbc",
   "metadata": {},
   "source": [
    "# What is a Neural Network?\n",
    "\n",
    "A neural network is a computer model inspired by the human brain.\n",
    "\n",
    "‚úîÔ∏è Basic idea:\n",
    "\n",
    "- The brain has **neurons** that pass signals to each other.\n",
    "- A neural network has artificial neurons, which are just **math** functions.\n",
    "- Each connection has a **weight** (a number the network learns).\n",
    "\n",
    "‚úîÔ∏è Goal:\n",
    "\n",
    "Neural networks learn patterns.\n",
    "\n",
    "Example:\n",
    "\n",
    "- Show pictures of cats ‚Üí it learns to recognize cats\n",
    "- Show a lot of English sentences ‚Üí it learns grammar patterns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebdbf715",
   "metadata": {},
   "source": [
    "# Early Neural Networks\n",
    "\n",
    "Early neural networks were small and could only solve simple problems.\n",
    "\n",
    "Types included:\n",
    "\n",
    "- Perceptrons (**1950s**): basic binary classifiers\n",
    "- Feedforward networks (1980s‚Äì1990s)\n",
    "- Convolutional Neural Networks (CNNs) for images\n",
    "- Recurrent Neural Networks (RNNs) for sequences\n",
    "- ...\n",
    "\n",
    "They worked‚Ä¶ but not very well for long text.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/perceptron.png\" width=\"300\">\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e023877e",
   "metadata": {},
   "source": [
    "# Deep Learning\n",
    "Deep learning ‚Üí Neural networks with **many layers** (‚Äúdeep‚Äù networks)\n",
    "A deep learning model learns by adjusting millions or billions of internal numbers called **weights**.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/deep_network.png\" width=\"300\">\n",
    "</p>\n",
    "\n",
    "\n",
    "## Why the word \"deep\"?\n",
    "Because the neural network has many layers stacked on top of each other:\n",
    "Input ‚Üí Layer 1 ‚Üí Layer 2 ‚Üí Layer 3 ‚Üí ‚Ä¶ ‚Üí Layer N ‚Üí Output\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57c9ecae",
   "metadata": {},
   "source": [
    "# ‚ö° 1. The Breakthrough: Transformers (2017)\n",
    "In 2017, **Google** published a paper called:\n",
    "\n",
    "**\"Attention is All You Need\"**\n",
    "\n",
    "This introduced the Transformer architecture.\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/transformer.png\" width=\"300\">\n",
    "</p>\n",
    "\n",
    "\n",
    "‚úîÔ∏è What made Transformers special?\n",
    "\n",
    "Attention mechanism.\n",
    "\n",
    "It lets the model:\n",
    "\n",
    "- Look at all words at once\n",
    "- Decide which words are important\n",
    "- Remember long-range relationships\n",
    "\n",
    "Example:\n",
    "\n",
    "In the sentence\n",
    "‚ÄúThe cat that chased the dog was hungry.‚Äù\n",
    "The model can link ‚Äúcat‚Äù ‚Üî ‚Äúwas hungry‚Äù, even though they are far apart.\n",
    "This solved the ‚Äúforgetting‚Äù problem.\n",
    "\n",
    "‚úîÔ∏è Why Transformers enabled huge models:\n",
    "\n",
    "- They can process text in parallel\n",
    "- They scale extremely well with more data\n",
    "- They learn long-range meaning and structure\n",
    "\n",
    "This architecture is the foundation of all modern LLMs:\n",
    "- GPT series (OpenAI)\n",
    "- Llama (Meta)\n",
    "- Gemini (Google)\n",
    "- Claude (Anthropic)\n",
    "- Mistral models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "457e581b",
   "metadata": {},
   "source": [
    "# ‚öôÔ∏è 2. GPU?\n",
    "\n",
    "A **GPU (Graphics Processing Unit)** is a special type of processor originally designed for one job:\n",
    "\n",
    "üëâ Render graphics for video games.\n",
    "\n",
    "To do this, a GPU needs to perform millions of small math operations in parallel (at the same time).\n",
    "This ‚Äúparallel processing‚Äù is EXACTLY what neural networks need.\n",
    "\n",
    "Neural networks rely heavily on:\n",
    "\n",
    "- matrix multiplication\n",
    "- vector operations\n",
    "- linear algebra\n",
    "\n",
    "GPUs accelerate these operations thousands of times faster than CPUs.\n",
    "\n",
    "Before around 2010, GPUs were mostly for gaming.\n",
    "\n",
    "Then a breakthrough happened:\n",
    "- Nvidia created CUDA (2006), allowing GPUs to run general-purpose math\n",
    "- Researchers discovered GPUs dramatically speed up neural networks\n",
    "- Deep learning started exploding\n",
    "\n",
    "Without GPUs:\n",
    "- Training an LLM would take hundreds of years\n",
    "- Transforming billions of parameters would be impossible\n",
    "\n",
    "Today‚Äôs massive GPU clusters (thousands of GPUs working together) make LLM training feasible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bd891f",
   "metadata": {},
   "source": [
    "# üåê 3. Huge Amounts of Data\n",
    "\n",
    "Early neural networks had:\n",
    "\n",
    "- tiny datasets\n",
    "- low-quality text sources\n",
    "- limited access to the internet\n",
    "\n",
    "To train an LLM you need massive amounts of text, such as:\n",
    "\n",
    "- billions of web pages\n",
    "- books\n",
    "- Wikipedia\n",
    "- forums\n",
    "- code repositories\n",
    "\n",
    "This scale of data did not exist or was not easily accessible 20 years ago.\n",
    "\n",
    "Thanks to the internet age, researchers can now build datasets containing trillions of tokens."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6190bcbe",
   "metadata": {},
   "source": [
    "### üöÄ When these three things came together, LLMs became possible\n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"images/chatGPT.png\" width=\"300\">\n",
    "</p>\n",
    "\n",
    "A **generative pre-trained transformer (GPT)** is a type of **large language model (LLM)** that is widely used in generative AI chatbots.GPTs are based on a **deep learning** architecture called the **transformer**. They are **pre-trained** on large datasets of unlabeled content, and able to generate novel content.\n",
    "\n",
    "Parameters:\n",
    "- A perceptron with 2 inputs -> 3 parameters\n",
    "- GPT-3 -> 175 billion parameters\n",
    "- GPT-5 -> 1.7‚Äì1.8 trillion parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909732a1",
   "metadata": {},
   "source": [
    "## üß† Hallucination?\n",
    "\n",
    "In the context of Large Language Models (LLMs) like ChatGPT:\n",
    "\n",
    "Hallucination means the model makes up information that is false, inaccurate, or completely fabricated, but presents it as if it were true.\n",
    "\n",
    "‚úîÔ∏è Examples:\n",
    "\n",
    "- Fictional facts:\n",
    "\n",
    "User: ‚ÄúWho won the Nobel Prize in 2020 for physics?‚Äù\n",
    "Model: ‚ÄúIt was Dr. John Smith.‚Äù (Incorrect)\n",
    "\n",
    "- Made-up citations:\n",
    "\n",
    "User: ‚ÄúGive me a reference for this topic.‚Äù\n",
    "Model: ‚ÄúDoe, J. (2019). Advanced AI Studies. Journal of AI.‚Äù (Doesn‚Äôt exist)\n",
    "\n",
    "- Confident but wrong reasoning:\n",
    "\n",
    "User: ‚ÄúExplain how unicorns fly.‚Äù\n",
    "Model: Provides a plausible-sounding explanation even though unicorns aren‚Äôt real.\n",
    "\n",
    "## ‚öôÔ∏è Why Do LLMs Hallucinate?\n",
    "\n",
    "LLMs like GPT are not **‚Äúthinking machines‚Äù** ‚Äî they are pattern predictors. They generate text based on statistical patterns learned from data.\n",
    "\n",
    "Key reasons:\n",
    "### 1Ô∏è‚É£ They predict the most likely next word\n",
    "\n",
    "LLMs are trained to continue text plausibly, not necessarily correctly.\n",
    "They don‚Äôt have a true understanding of facts‚Äîthey just generate what sounds right.\n",
    "\n",
    "Analogy:\n",
    "Imagine a very smart autocomplete keyboard‚Äîit will suggest the most probable next word, even if it‚Äôs wrong.\n",
    "\n",
    "### 2Ô∏è‚É£ Training data is imperfect\n",
    "\n",
    "LLMs learn from text on the internet, books, code, articles‚Ä¶\n",
    "Some of that data is wrong, biased, or fictional.\n",
    "The model can reproduce errors from the data.\n",
    "\n",
    "### 3Ô∏è‚É£ Lack of real-world grounding\n",
    "\n",
    "LLMs don‚Äôt access real-time data (unless connected to a knowledge base or plugin).\n",
    "They can‚Äôt check facts on their own‚Äîthey rely on patterns they learned during training.\n",
    "\n",
    "### 4Ô∏è‚É£ Ambiguity in the prompt\n",
    "\n",
    "If the user asks vague or creative questions, the model may generate plausible-sounding but incorrect answers.\n",
    "\n",
    "Example: ‚ÄúExplain how humans breathe underwater‚Äù ‚Üí generates imaginative explanation, because it tries to be helpful even if impossible."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4df2c6d6",
   "metadata": {},
   "source": [
    "# Connecting to OpenAI \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d25278",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "from dotenv import load_dotenv\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "from openai import OpenAI\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "600075de",
   "metadata": {},
   "outputs": [],
   "source": [
    "load_dotenv(override=True)\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "# Check the key\n",
    "\n",
    "if not api_key:\n",
    "    print(\"No API key was found!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c977f2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "openai = OpenAI()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61f54f1a",
   "metadata": {},
   "source": [
    "# Frontier model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bfd86b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "message = \"Hello, GPT! This is my first ever message to you!\"\n",
    "response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=[{\"role\":\"user\", \"content\":message}])\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a17af4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class to represent a Webpage\n",
    "# If you're not familiar with Classes, check out the \"Intermediate Python\" notebook\n",
    "\n",
    "# Some websites need you to use proper headers when fetching them:\n",
    "headers = {\n",
    " \"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/117.0.0.0 Safari/537.36\"\n",
    "}\n",
    "\n",
    "class Website:\n",
    "\n",
    "    def __init__(self, url):\n",
    "        \"\"\"\n",
    "        Create this Website object from the given url using the BeautifulSoup library\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        response = requests.get(url, headers=headers)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            irrelevant.decompose()\n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13f85685",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's try one out. Change the website and add print statements to follow along.\n",
    "\n",
    "person = Website(\"https://fa.wikipedia.org/wiki/%D8%AD%D8%A7%D9%81%D8%B8\")\n",
    "print(person.title)\n",
    "print(person.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "201a160e",
   "metadata": {},
   "source": [
    "## Types of prompts\n",
    "\n",
    "Models like GPT4o have been trained to receive instructions in a particular way.\n",
    "\n",
    "They expect to receive:\n",
    "\n",
    "**A system prompt** that tells them what task they are performing and what tone they should use\n",
    "\n",
    "**A user prompt** -- the conversation starter that they should reply to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0deb63eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our system prompt - you can experiment with this later, changing the last sentence to 'Respond in markdown in Spanish.\"\n",
    "\n",
    "system_prompt = \"You are an assistant that analyzes the contents of a website \\\n",
    "and provides a short summary, ignoring text that might be navigation related. \\\n",
    "Respond in markdown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1258e31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that writes a User Prompt that asks for summaries of websites:\n",
    "\n",
    "def user_prompt_for(website):\n",
    "    user_prompt = f\"You are looking at a website titled {website.title}\"\n",
    "    user_prompt += \"\\nThe contents of this website is as follows; \\\n",
    "please provide a short summary of this website in markdown. \\\n",
    "If it includes news or announcements, then summarize these too.\\n\\n\"\n",
    "    user_prompt += website.text\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f2a54e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(user_prompt_for(person))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c9f6252",
   "metadata": {},
   "source": [
    "## Messages\n",
    "\n",
    "The API from OpenAI expects to receive messages in a particular structure.\n",
    "Many of the other APIs share this structure:\n",
    "\n",
    "```python\n",
    "[\n",
    "    {\"role\": \"system\", \"content\": \"system message goes here\"},\n",
    "    {\"role\": \"user\", \"content\": \"user message goes here\"}\n",
    "]\n",
    "```\n",
    "To give you a preview, the next 2 cells make a rather simple call - we won't stretch the mighty GPT (yet!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01d2acf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"ÿ¥ŸÖÿß €å⁄© ÿØÿ≥ÿ™€åÿßÿ± ÿ¥ŸàÿÆ ÿ∑ÿ®ÿπ Ÿáÿ≥ÿ™€åÿØ\"},\n",
    "    {\"role\": \"user\", \"content\": \"€≤ + €≤ ⁄ÜŸÜÿØ ŸÖ€åÿ¥Ÿáÿü\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fcbc557",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To give you a preview -- calling OpenAI with system and user messages:\n",
    "\n",
    "response = openai.chat.completions.create(model=\"gpt-4o-mini\", messages=messages)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13621b94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how this function creates exactly the format above\n",
    "\n",
    "def messages_for(website):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(website)}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7543096",
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_for(person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf2a5c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now: call the OpenAI API. You will get very familiar with this!\n",
    "\n",
    "def summarize(url):\n",
    "    website = Website(url)\n",
    "    response = openai.chat.completions.create(\n",
    "        model = \"gpt-4o-mini\",\n",
    "        messages = messages_for(website)\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f84fd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize(\"https://fa.wikipedia.org/wiki/%D8%AD%D8%A7%D9%81%D8%B8\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42b0ac14",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to display this nicely in the Jupyter output, using markdown\n",
    "\n",
    "def display_summary(url):\n",
    "    summary = summarize(url)\n",
    "    display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a83470",
   "metadata": {},
   "outputs": [],
   "source": [
    "display_summary(\"https://fa.wikipedia.org/wiki/%D8%AD%D8%A7%D9%81%D8%B8\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956df924",
   "metadata": {},
   "source": [
    "# Open Source model\n",
    "**Benefits:**\n",
    "1. No API charges - open-source\n",
    "2. Data doesn't leave your box\n",
    "\n",
    "**Disadvantages:**\n",
    "1. Significantly less power than Frontier Model\n",
    "\n",
    "## Recap on installation of Ollama\n",
    "\n",
    "Simply visit [ollama.com](https://ollama.com) and install!\n",
    "\n",
    "Once complete, the ollama server should already be running locally.  \n",
    "If you visit:  \n",
    "[http://localhost:11434/](http://localhost:11434/)\n",
    "\n",
    "You should see the message `Ollama is running`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e76d4cce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83e7eb61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d05b32f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a messages list using the same format that we used for OpenAI\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Describe some of the business applications of Generative AI\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85be5651",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e1fce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's just make sure the model is loaded\n",
    "\n",
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d41b9ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# If this doesn't work for any reason, try the 2 versions in the following cells\n",
    "# And double check the instructions in the 'Recap on installation of Ollama' at the top of this lab\n",
    "# And if none of that works - contact me!\n",
    "\n",
    "response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)\n",
    "print(response.json()['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "422d2246",
   "metadata": {},
   "source": [
    "# Avalai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2028e2cd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
